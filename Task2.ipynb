{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOqn8gc1UUry"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "try:\n",
        "    dataset = pd.read_csv('/content/Churn_Modelling.csv')\n",
        "    original_df=dataset.copy()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'Churn_Modelling.csv' was not found.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "dataset.head()\n",
        "\n",
        "dataset.shape\n",
        "\n",
        "dataset.dtypes\n",
        "\n",
        "# Drop irrelevant columns as they do not provide value for the model\n",
        "dataset.drop(columns=['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)\n",
        "print(\"Remaining columns:\\n\")\n",
        "dataset.columns.to_list()\n",
        "\n",
        "obj_columns=['Gender','Geography']\n",
        "for column in obj_columns:\n",
        "    dataset[column] = dataset[column].astype('category')\n",
        "\n",
        "dataset.dtypes\n",
        "\n",
        "dataset.isnull().sum()\n",
        "\n",
        "for col in dataset.columns:\n",
        "    print(col,\" : \",dataset[col].unique())\n",
        "    print()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "dataset.hist(figsize=(12,8))\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "#Count Plots for Categorical Features\n",
        "for col in ['Gender', 'Geography', 'HasCrCard', 'IsActiveMember', 'NumOfProducts']:\n",
        "    sns.countplot(x=col, data=dataset)\n",
        "    plt.show()\n",
        "\n",
        "print(\"Churn Rate:\\n\", dataset['Exited'].value_counts(normalize=True))\n",
        "sns.countplot(x='Exited', data=dataset)\n",
        "plt.title(\"Churn Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Churn vs Gender\n",
        "sns.countplot(x='Gender', hue='Exited', data=dataset)\n",
        "plt.title(\"Churn by Gender\")\n",
        "plt.show()\n",
        "\n",
        "# Churn vs Geography\n",
        "sns.countplot(x='Geography', hue='Exited', data=dataset)\n",
        "plt.title(\"Churn by Geography\")\n",
        "plt.show()\n",
        "\n",
        "# Correlation Heatmap\n",
        "corr = dataset.corr(numeric_only=True)\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Boxplots of Numerical vs Churn\n",
        "for col in ['Age','Balance','CreditScore','EstimatedSalary']:\n",
        "    sns.boxplot(x='Exited', y=col, data=dataset)\n",
        "    plt.title(f\"{col} vs Churn\")\n",
        "    plt.show()\n",
        "\n",
        "# Separate features and target\n",
        "X = dataset.drop('Exited', axis=1)\n",
        "y = dataset['Exited']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Gender', 'Geography']\n",
        "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
        "\n",
        "\n",
        "# One-Hot Encode categorical features (drop first to avoid dummy variable trap)\n",
        "X = pd.get_dummies(X, columns=['Gender', 'Geography'], drop_first=True)\n",
        "\n",
        "\n",
        "print(\"After encoding:\")\n",
        "X.head()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "\n",
        "X['Gender_Male'] = X['Gender_Male'].astype(int)\n",
        "X['Geography_Germany'] = X['Geography_Germany'].astype(int)\n",
        "X['Geography_Spain'] = X['Geography_Spain'].astype(int)\n",
        "\n",
        "\n",
        "print(\"After scaling:\")\n",
        "X.head()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "log_model = LogisticRegression(max_iter=1000)\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\")\n",
        "print(accuracy_score(y_test, y_pred_log))\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(\"\\nRandom Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "print(\"\\nXGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
        "\n",
        "# Make Predictions & Probabilities\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "y_prob_log = log_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_prob_rf = rf_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_prob_xgb = xgb_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "#Add CustomerId from original_df (align with X_test indices)\n",
        "customer_ids = original_df.loc[X_test.index, \"CustomerId\"]\n",
        "\n",
        "#Export Predictions + Probabilities with CustomerId\n",
        "results = pd.DataFrame({\n",
        "    \"CustomerId\": customer_ids,\n",
        "    \"Actual\": y_test,\n",
        "    \"LogReg_Pred\": y_pred_log,\n",
        "    \"LogReg_Prob\": y_prob_log,\n",
        "    \"RF_Pred\": y_pred_rf,\n",
        "    \"RF_Prob\": y_prob_rf,\n",
        "    \"XGB_Pred\": y_pred_xgb,\n",
        "    \"XGB_Prob\": y_prob_xgb\n",
        "})\n",
        "\n",
        "results.to_csv(\"churn_predictions.csv\", index=False)\n",
        "\n",
        "# Logistic Regression\n",
        "log_importances = pd.DataFrame({\n",
        "    \"Feature\": X_train.columns,\n",
        "    \"Importance\": log_model.coef_[0]\n",
        "}).sort_values(by=\"Importance\", key=abs, ascending=False)\n",
        "log_importances.to_csv(\"log_reg_feature_importance.csv\", index=False)\n",
        "\n",
        "# Random Forest\n",
        "rf_importances = pd.DataFrame({\n",
        "    \"Feature\": X_train.columns,\n",
        "    \"Importance\": rf_model.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "rf_importances.to_csv(\"rf_feature_importance.csv\", index=False)\n",
        "\n",
        "# XGBoost\n",
        "xgb_importances = pd.DataFrame({\n",
        "    \"Feature\": X_train.columns,\n",
        "    \"Importance\": xgb_model.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "xgb_importances.to_csv(\"xgb_feature_importance.csv\", index=False)"
      ]
    }
  ]
}